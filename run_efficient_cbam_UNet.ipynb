{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from model.unet import Model      # UNet\n",
    "# from model.efficientnet_unet import Model  # EfficientNet (Encoder) + UNet (Decoder)\n",
    "from model.efficientnet_cbam_unet import Model  # EfficientNet (Encoder) + CBAM + UNet (Decoder)\n",
    "\n",
    "from dataset.dataset import Dataset\n",
    "from utils.visualize import visualize_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define paths\n",
    "CHECKPOINT_PATH = ''\n",
    "TRAIN_INDEX_PATH = 'data/train_toy_idx.txt'\n",
    "TEST_INDEX_PATH = 'data/test_toy_idx.txt'\n",
    "\n",
    "# Define modes\n",
    "MODE = 'train'                              # train/test\n",
    "RECORD = False                              # Record predictions in wandb\n",
    "SAVE_MODEL_NAME = 'EfficientNet_CBAM_UNet'  # Name of model to save\n",
    "\n",
    "# Define hyperparameters\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4                              # While MODE = 'test', remember to set BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup WanDB for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_current_timestamp():\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "    return timestamp\n",
    "\n",
    "timestamp = get_current_timestamp()\n",
    "log_dir = f'checkpoint/{timestamp}_{SAVE_MODEL_NAME}'\n",
    "\n",
    "if RECORD and MODE == 'train':\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Face_Segmentation\",\n",
    "        name=f'{timestamp}_{SAVE_MODEL_NAME}',\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"CelebAMaskHQ\",\n",
    "        \"epochs\": 30,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Current Working Directory to current location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change current working directory to file location\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(current_folder)\n",
    "\n",
    "# Change to the below code if in python file\n",
    "# os.chdir(os.path.dirname(__file__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create U-Net model\n",
    "model = Model(3, 19)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print total number of parameters\n",
    "print(f'Total number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = Dataset(id_file=TRAIN_INDEX_PATH, transform=ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = Dataset(id_file=TEST_INDEX_PATH, transform=ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer):\n",
    "    lowest_test_loss = 1e10\n",
    "    if CHECKPOINT_PATH != '':\n",
    "        model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "        print('Loaded model from checkpoint.')\n",
    "    model.train()\n",
    "    all_train_loss = []\n",
    "    all_test_loss = []\n",
    "    for epoch in tqdm(range(EPOCHS), leave=True):\n",
    "        \n",
    "        for images, masks in tqdm(train_loader, leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).squeeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            train_loss = criterion(outputs, masks)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(test_loader, leave=False):\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device).squeeze(1)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                test_loss = criterion(outputs, masks)\n",
    "\n",
    "                # Save model with lowest test loss\n",
    "                if test_loss < lowest_test_loss:\n",
    "                    lowest_test_loss = test_loss\n",
    "                    if not os.path.exists(log_dir) and MODE == 'train':\n",
    "                        os.makedirs(log_dir)\n",
    "                    if os.path.exists(f'{log_dir}/{SAVE_MODEL_NAME}.pth'):\n",
    "                        os.rename(f'{log_dir}/{SAVE_MODEL_NAME}.pth', f'{log_dir}/{SAVE_MODEL_NAME}_{get_current_timestamp()}.pth')\n",
    "                    torch.save(model.state_dict(), f'{log_dir}/{SAVE_MODEL_NAME}.pth')\n",
    "\n",
    "            tqdm.write(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "            all_train_loss.append(train_loss.item())\n",
    "            all_test_loss.append(test_loss.item())\n",
    "\n",
    "        if RECORD:\n",
    "            wandb.log({\"train_loss\": train_loss.item(), \"test_loss\": test_loss.item()})\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(all_train_loss, label='Training loss')\n",
    "    plt.plot(all_test_loss, label='Testing loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{log_dir}/loss.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, masks in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).squeeze(1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        ## Visualize predictions with first image\n",
    "        visualize_predictions(images, predicted, masks)\n",
    "        # break\n",
    "\n",
    "    print(f'Test Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_celeb = ['background', 'skin', 'nose',\n",
    "                'eye_g', 'l_eye', 'r_eye', 'l_brow',\n",
    "                'r_brow', 'l_ear', 'r_ear', 'mouth',\n",
    "                'u_lip', 'l_lip', 'hair', 'hat',\n",
    "                'ear_r', 'neck_l', 'neck', 'cloth']\n",
    "\n",
    "\n",
    "def read_mask(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if type(img) is type(None):\n",
    "        return np.zeros((256, 256, 1), dtype=np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def mask2binary(path):\n",
    "    mask = read_mask(path)\n",
    "    mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "    mask = np.where(mask > 0, 1, 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def rle_encode(img):\n",
    "    pixels = img.flatten()\n",
    "    if np.sum(pixels) == 0:\n",
    "        return '0'\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    # to string sep='_'\n",
    "    runs = '_'.join(str(x) for x in runs)\n",
    "    return runs\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split('_')\n",
    "    s = [0 if x == '' else int(x) for x in s]\n",
    "    if np.sum(s) == 0:\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    starts, lengths = [np.asarray(x, dtype=int)\n",
    "                       for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 255\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "def mask2csv(mask_paths, csv_path='mask.csv', image_id=1, header=False):\n",
    "    \"\"\"\n",
    "        mask_paths: dict of label:mask_paths\n",
    "        ['label1':path1,'label2':path2,...]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, label in enumerate(labels_celeb):\n",
    "        try:\n",
    "            mask = mask2binary(mask_paths[label])\n",
    "        except:\n",
    "            mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "        mask = rle_encode(mask)\n",
    "        results.append(mask)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.insert(0, 'label', labels_celeb)\n",
    "    df.insert(0, 'Usage', [\"Public\" for i in range(len(results))])\n",
    "    df.insert(0, 'ID', [image_id*19+i for i in range(19)])\n",
    "    if header:\n",
    "        df.columns = ['ID', 'Usage', 'label', 'segmentation']\n",
    "    # print()\n",
    "    # print(df)\n",
    "    df.to_csv(csv_path, mode='a', header=header, index=False)\n",
    "\n",
    "\n",
    "def mask2csv2(masks, csv_path='mask.csv', image_id=1, header=False):\n",
    "    \"\"\"\n",
    "        mask_paths: dict of label:mask\n",
    "        ['label1':mask1,'label2':mask2,...]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, label in enumerate(labels_celeb):\n",
    "        try:\n",
    "            mask = masks[label]\n",
    "        except:\n",
    "            mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "        mask = rle_encode(mask)\n",
    "        results.append(mask)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.insert(0, 'label', labels_celeb)\n",
    "    df.insert(0, 'Usage', [\"Public\" for i in range(len(results))])\n",
    "    df.insert(0, 'ID', [image_id*19+i for i in range(19)])\n",
    "\n",
    "    if header:\n",
    "        df.columns = ['ID', 'Usage', 'label', 'segmentation']\n",
    "    # print()\n",
    "    # print(df)\n",
    "    df.to_csv(csv_path, mode='a', header=header, index=False)\n",
    "\n",
    "def export():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "if MODE == 'train':\n",
    "    train(model, train_loader, test_loader, criterion, optimizer)\n",
    "elif MODE == 'test':\n",
    "    test(model, test_loader)\n",
    "elif MODE == 'csv':\n",
    "    export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
